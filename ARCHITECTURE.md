# ðŸ—ï¸ System Architecture & File Structure

This document provides a deep dive into the technical architecture, file organization, and data flow of **RagBot 2.0**.

---

## ðŸ“‚ 1. Professional File Structure

The project follows a **Micro-Service Style** Monolith, separating concerns clearly between the Presentation Layer (Client) and the Logic/Data Layer (Server).

```m
RagBot-2.0/
â”œâ”€â”€ ðŸ“„ README.md                  # Project overview and automated setup guide
â”œâ”€â”€ ðŸ“„ ARCHITECTURE.md            # Technical design documentation (This file)
â”‚
â”œâ”€â”€ ðŸ“¦ client/                    # [PRESENTATION LAYER] Streamlit Application
â”‚   â”œâ”€â”€ ðŸ“„ app.py                 # ðŸŸ¢ Entry Point: Main UI Thread & Layout Orchestrator
â”‚   â”œâ”€â”€ ðŸ“„ config.py              # âš™ï¸ Configuration: API Endpoints & Global Constants
â”‚   â”œâ”€â”€ ðŸ“„ requirements.txt       # ðŸ“¦ Dependencies: Frontend-specific libraries
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“‚ components/            # ðŸ§© UI Modules (Atomic Design Pattern)
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ chat.py            #    - Renders chat bubbles & message history
â”‚   â”‚   â””â”€â”€ ðŸ“„ upload.py          #    - Handles file drag-and-drop & sidebar logic
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ“‚ utils/                 # ðŸ› ï¸ Client Utilities
â”‚       â””â”€â”€ ðŸ“„ api.py             #    - API Wrapper (Requests) to communicate with Backend
â”‚
â””â”€â”€ ðŸ“¦ server/                    # [LOGIC LAYER] FastAPI Application
    â”œâ”€â”€ ðŸ“„ main.py                # ðŸŸ¢ Entry Point: API Routes (/ask, /upload) & Middleware
    â”œâ”€â”€ ðŸ“„ verify_rag.py          # ðŸ§ª System Check: Verifies DB & LLM connectivity
    â”œâ”€â”€ ðŸ“„ requirements.txt       # ðŸ“¦ Dependencies: Backend, AI & DB libraries
    â”œâ”€â”€ ðŸ“„ .env                   # ðŸ”‘ Secrets: API Keys (e.g., GROQ_API_KEY)
    â”‚
    â”œâ”€â”€ ðŸ“‚ modules/               # ðŸ§  Domain Logic (The "Brain")
    â”‚   â”œâ”€â”€ ðŸ“„ load_vectorstore.py #    - ETL: PDF Parser -> Chunking -> Embedding -> ChromaDB
    â”‚   â”œâ”€â”€ ðŸ“„ llm.py              #    - AI: Llama-3.3 Configuration & Prompt Handling
    â”‚   â””â”€â”€ ðŸ“„ query_handlers.py   #    - Service: Glues Retrieval + LLM together
    â”‚
    â”œâ”€â”€ ðŸ“‚ chroma_db/             # ðŸ’¾ Data Persistence: Local Vector Database files
    â””â”€â”€ ðŸ“‚ uploaded_pdfs/         # ðŸ“‚ Temp Storage: Raw uploaded files
```

---

## ðŸ”„ 2. Data Flow Architecture

### 2.1 Ingestion Pipeline (PDF to Vector)
When a user uploads a file, it goes through the `load_vectorstore.py` ETL process:

```mermaid
sequenceDiagram
    participant User
    participant Client
    participant API as Main.py
    participant ETL as load_vectorstore.py
    participant DB as ChromaDB

    User->>Client: Uploads PDF
    Client->>API: POST /upload_pdfs
    API->>ETL: Trigger Ingestion
    ETL->>ETL: Parse Text (PyPDFLoader)
    ETL->>ETL: Chunk Text (RecursiveSplitter)
    ETL->>ETL: Generate Embeddings (HuggingFace)
    ETL->>DB: Upsert Vectors
    DB-->>API: Success
    API-->>Client: 200 OK
    Client-->>User: "File Analyzed"
```

### 2.2 RAG Query Pipeline (Question to Answer)
When a user asks a question, the `query_handlers.py` logic takes over:

```mermaid
sequenceDiagram
    participant User
    participant Client
    participant API as Main.py
    participant Service as query_handlers.py
    participant DB as ChromaDB
    participant LLM as Groq API

    User->>Client: "What is this diagnosis?"
    Client->>API: POST /ask (query)
    API->>Service: Handle Query
    Service->>DB: Similarity Search (k=3)
    DB-->>Service: Return Top 3 Document Contexts
    Service->>LLM: Send Prompt + Context + Query
    LLM-->>Service: Generated Answer
    Service-->>API: Response
    API-->>Client: JSON Response
    Client-->>User: Displays Message
```

---

## ðŸ§© 3. Key Components

### **Vector Database (ChromaDB)**
- **Role**: Long-term memory of the AI.
- **Location**: `./server/chroma_db`
- **Why**: Runs locally, zero latency, no API keys required, persists data across restarts.

### **Embedding Model (all-MiniLM-L6-v2)**
- **Role**: Translates text into numbers (vectors).
- **Location**: Cached locally via `langchain-huggingface`.
- **Performance**: Optimized for CPU usage, ensuring the server runs smoothly on standard laptops.

### **LLM Inference (Groq Llama 3.3)**
- **Role**: The "Reasoning Engine".
- **Why**: Groq's LPU hardware delivers 300+ tokens/sec, making the chat experience feel instantaneous.
